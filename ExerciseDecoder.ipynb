{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d602e75d",
   "metadata": {},
   "source": [
    "# 0. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d97e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import itertools\n",
    "import absl.logging\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix,confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.layers import (LSTM, Dense, Dropout,Input, Flatten, Bidirectional, Permute, multiply)\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "tf.autograph.set_verbosity(1)\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f470e2",
   "metadata": {},
   "source": [
    "# 1. USING MEDIA PIPE FOR DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41af076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "    image.flags.writeable = False                  \n",
    "    results = model.process(image)                 \n",
    "    image.flags.writeable = True                   \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "20cde117",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_module = mp.solutions.pose\n",
    "draw_lines = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bd7ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    draw_lines.draw_landmarks(image, results.pose_landmarks, pose_module.POSE_CONNECTIONS,\n",
    "                                draw_lines.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                draw_lines.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ebe952",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) \n",
    "HEIGHT = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) \n",
    "WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) \n",
    "print(HEIGHT)\n",
    "print(WIDTH)\n",
    "FPS = 29\n",
    "with pose_module.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()   \n",
    "        image, results = mp_detection(frame, pose)\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "        except:\n",
    "            pass\n",
    "        draw_landmarks(image, results)               \n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb32f3",
   "metadata": {},
   "source": [
    "# 2. KEYPOINTS EXTRACTION FOR DATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a81823f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = []\n",
    "for i in results.pose_landmarks.landmark:\n",
    "    test = np.array([i.x, i.y, i.z, i.visibility])\n",
    "    pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd92eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keypoints are 33 landmarks which will have 4 values (x, y, z, visibility)\n",
    "num_landmarks = len(landmarks)\n",
    "num_values = len(test)\n",
    "num_input_values = num_landmarks*num_values\n",
    "print(num_landmarks,num_values,num_input_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f4d1079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b907e8",
   "metadata": {},
   "source": [
    "# 3. DATA COLLECTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ddcaecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tsaiajay/Downloads/Project/data\n"
     ]
    }
   ],
   "source": [
    "#### RUN THIS WHEN USING PRE EXISTING DATA\n",
    "\n",
    "DATA_PATH = os.path.join(os. getcwd(),'data') \n",
    "print(DATA_PATH)\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "\n",
    "actions = np.array(['curl','press','squat','butbrdige','leglift','situps'])\n",
    "colors = [(245,117,16), (117,245,16),(16,117,245),(127, 0, 255),(255, 255, 0),(255,192,203)]\n",
    "num_classes = len(actions)\n",
    "\n",
    "no_sequences = 1\n",
    "sequence_length = FPS*1\n",
    "start_folder = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee0f9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## DONT RUN THIS IF YOU RUN THE ABOVE CELL\n",
    "###### used for dataset collection #######\n",
    "#####  RUN THIS when NEED TO ADD NEW EXCERSICE OR NEW DAta\n",
    "\n",
    "\n",
    "# DATA_PATH = os.path.join(os. getcwd(),'data') \n",
    "# print(DATA_PATH)\n",
    "# if not os.path.exists(DATA_PATH):\n",
    "#     os.makedirs(DATA_PATH)\n",
    "# actions = np.array(['situps'])\n",
    "# colors =[(255,192,203)]\n",
    "# num_classes = len(actions)\n",
    "\n",
    "# no_sequences = 30\n",
    "# sequence_length = FPS*1\n",
    "# start_folder = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fed6b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####DONT RUN IF YOU HAVE DATA EXISTS\n",
    "###### used for dataset collection #######\n",
    "\n",
    "\n",
    "######RUN IF YOU NEED TO GET MORE data\n",
    "\n",
    "# for action in actions:     \n",
    "#     for sequence in range(start_folder,no_sequences+start_folder):\n",
    "#         try: \n",
    "#             os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))  \n",
    "#         except:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7622b573",
   "metadata": {},
   "source": [
    "# 4. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b81490",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########DONT RUN IF YOU ALREADY HAVE DATA #############\n",
    "###### used for dataset collection #######\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# with pose_module.Pose(min_detection_confidence=0.6, min_tracking_confidence=0.6) as pose:\n",
    "#     for idx, action in enumerate(actions):\n",
    "#         for sequence in range(start_folder, start_folder+no_sequences):\n",
    "#             for frame_num in range(sequence_length):\n",
    "#                 ret, frame = cap.read()\n",
    "#                 image, results = mp_detection(frame, pose)\n",
    "#                 try:\n",
    "#                     landmarks = results.pose_landmarks.landmark\n",
    "#                 except:\n",
    "#                     print(\"INTO EXCEPT\")\n",
    "#                     pass\n",
    "                \n",
    "#                 draw_landmarks(image, results) \n",
    "                \n",
    "#                 if frame_num == 0: \n",
    "#                     cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "#                     cv2.putText(image, 'Collecting {} Video # {}'.format(action, sequence), (15,30), \n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 8, cv2.LINE_AA)\n",
    "#                     cv2.putText(image, 'Collecting {} Video # {}'.format(action, sequence), (15,30), \n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 1, colors[idx], 4, cv2.LINE_AA)\n",
    "#                     cv2.imshow('OpenCV Feed', image)\n",
    "#                     cv2.waitKey(800)\n",
    "#                 else: \n",
    "#                     cv2.putText(image, 'Collecting {} Video # {}'.format(action, sequence), (15,30), \n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 8, cv2.LINE_AA)\n",
    "#                     cv2.putText(image, 'Collecting {} Video # {}'.format(action, sequence), (15,30), \n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 1, colors[idx], 4, cv2.LINE_AA)\n",
    "                    \n",
    "#                     cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "#                 keypoints = extract_keypoints(results)\n",
    "#                 npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "#                 np.save(npy_path, keypoints)\n",
    "#                 if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#                     break\n",
    "                    \n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b016129",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ec993",
   "metadata": {},
   "source": [
    "# 5. Preprocess Data and Create Labels/Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cad528c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0add3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in os.listdir(os.path.join(DATA_PATH, action)):\n",
    "        if os.path.isdir(os.path.join(DATA_PATH, action, sequence)) and not sequence.startswith('.'):\n",
    "            window = []\n",
    "            for frame_num in range(sequence_length):\n",
    "                i = np.load(os.path.join(DATA_PATH, action, sequence, \"{}.npy\".format(frame_num)))\n",
    "                window.append(i)\n",
    "                \n",
    "            sequences.append(window)\n",
    "            labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab459ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(615, 29, 132) (615, 6)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ac49993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(492, 29, 132) (492, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "print(X_train.shape, y_train.shape)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=15/90, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ae03d",
   "metadata": {},
   "source": [
    "# 6. Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "912f3153",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_cb = EarlyStopping(monitor='val_loss', min_delta=5e-4, patience=10, verbose=0, mode='min')\n",
    "l_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=0, mode='min')\n",
    "check_cb = ModelCheckpoint(filepath=DATA_PATH, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                 save_weights_only=False, mode='min', save_freq=1)\n",
    "\n",
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.01)\n",
    "\n",
    "batch_size = 64\n",
    "max_epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca0cad",
   "metadata": {},
   "source": [
    "## 6a. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "730f987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = f\"ExerciseRecognition-LSTM-{int(time.time())}\"\n",
    "log_dir = os.path.join(os.getcwd(), 'logs', NAME,'')\n",
    "tb_cb = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "callbacks = [tb_cb, e_cb, l_cb, check_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ae7595c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 29, 128)           133632    \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 29, 256)           394240    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 750150 (2.86 MB)\n",
      "Trainable params: 750150 (2.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(LSTM(128, return_sequences=True, activation='relu', input_shape=(sequence_length, num_input_values)))\n",
    "lstm.add(LSTM(256, return_sequences=True, activation='relu'))\n",
    "lstm.add(LSTM(128, return_sequences=False, activation='relu'))\n",
    "lstm.add(Dense(128, activation='relu'))\n",
    "lstm.add(Dense(64, activation='relu'))\n",
    "lstm.add(Dense(actions.shape[0], activation='softmax'))\n",
    "print(actions.shape[0])\n",
    "print(lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "lstm.fit(X_train, y_train, batch_size=batch_size, epochs=max_epochs, validation_data=(X_val, y_val), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58c4d8",
   "metadata": {},
   "source": [
    "## 6b. LSTM + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6e12666",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = f\"ExerciseRecognition-AttnLSTM-{int(time.time())}\"\n",
    "log_dir = os.path.join(os.getcwd(), 'logs', NAME,'')\n",
    "tb_cb = TensorBoard(log_dir=log_dir)\n",
    "callbacks = [tb_cb, e_cb, l_cb, check_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "07591dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(inputs, time_steps):\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    \n",
    "    # Luong's multiplicative score\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul') \n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c5c2e2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 29, 132)]            0         []                            \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 29, 512)              796672    ['input_2[0][0]']             \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " permute_1 (Permute)         (None, 512, 29)              0         ['bidirectional_1[0][0]']     \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 512, 29)              870       ['permute_1[0][0]']           \n",
      "                                                                                                  \n",
      " attention_vec (Permute)     (None, 29, 512)              0         ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      " attention_mul (Multiply)    (None, 29, 512)              0         ['bidirectional_1[0][0]',     \n",
      "                                                                     'attention_vec[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 14848)                0         ['attention_mul[0][0]']       \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 512)                  7602688   ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 512)                  0         ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 6)                    3078      ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8403308 (32.06 MB)\n",
      "Trainable params: 8403308 (32.06 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "H_units = 256\n",
    "input = Input(shape=(sequence_length, num_input_values))\n",
    "bi_lstm = Bidirectional(LSTM(H_units, return_sequences=True))(input)\n",
    "attention_layer = attention_block(bi_lstm, sequence_length)\n",
    "flattend_attention = Flatten()(attention_layer)\n",
    "dense_1 = Dense(2*H_units, activation='relu')(flattend_attention)\n",
    "dropout = Dropout(0.5)(dense_1)\n",
    "x = Dense(actions.shape[0], activation='softmax')(dropout)\n",
    "\n",
    "AttnLSTM = Model(inputs=[input], outputs=x)\n",
    "print(AttnLSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f988d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fee772cc7c0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AttnLSTM.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "AttnLSTM.fit(X_train, y_train, batch_size=batch_size, epochs=max_epochs, validation_data=(X_val, y_val), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b89f67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model map\n",
    "models = {\n",
    "    'LSTM_new': lstm, \n",
    "    'LSTM_Attention_new': AttnLSTM, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928f612",
   "metadata": {},
   "source": [
    "# 7a. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7647ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    save_dir = os.path.join(os.getcwd(), f\"{model_name}.h5\")\n",
    "    model.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fecf26",
   "metadata": {},
   "source": [
    "# 7b. Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed0114a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    load_dir = os.path.join(os.getcwd(), f\"{model_name}.h5\")\n",
    "    model.load_weights(load_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7747c6",
   "metadata": {},
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2101a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models.values():\n",
    "    i = model.predict(X_test, verbose=0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36c98e",
   "metadata": {},
   "source": [
    "# 9. Evaluations using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ecf242d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = {}\n",
    "eval_results['confusion matrix'] = None\n",
    "eval_results['accuracy'] = None\n",
    "eval_results['precision'] = None\n",
    "eval_results['recall'] = None\n",
    "eval_results['f1 score'] = None\n",
    "\n",
    "confusion_matrices = {}\n",
    "classification_accuracies = {}   \n",
    "precisions = {}\n",
    "recalls = {}\n",
    "f1_scores = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd4eafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=pyplot.cm.Blues):\n",
    "\n",
    "    pyplot.figure(figsize = (6,6))\n",
    "    pyplot.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    pyplot.title(title)\n",
    "    pyplot.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    pyplot.xticks(tick_marks, classes, rotation=90)\n",
    "    pyplot.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    cm = np.round(cm,2)\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        pyplot.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    pyplot.tight_layout()\n",
    "    pyplot.ylabel('True label')\n",
    "    pyplot.xlabel('Predicted label')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6778f",
   "metadata": {},
   "source": [
    "## 9a. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccbb90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    \n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    \n",
    "    confusion_matrices[model_name] = confusion_matrix(ytrue, yhat)\n",
    "    confusion_mtx  = confusion_matrix(ytrue, yhat)\n",
    "    cm = plot_confusion_matrix(confusion_mtx, classes = list(label_map.items()), normalize=False)\n",
    "    print(f\"{model_name} confusion matrix: {os.linesep}{confusion_matrices[model_name]}\")\n",
    "eval_results['confusion matrix'] = confusion_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c6dc5",
   "metadata": {},
   "source": [
    "## 9b. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36146f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    classification_accuracies[model_name] = accuracy_score(ytrue, yhat)   \n",
    "     \n",
    "    print(f\"{model_name} classification accuracy = {round(classification_accuracies[model_name]*100,3)}%\")\n",
    "\n",
    "eval_results['accuracy'] = classification_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33efa73a",
   "metadata": {},
   "source": [
    "## 9c. Precision, Recall, and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35067c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    \n",
    "    report = classification_report(ytrue, yhat, target_names=actions, output_dict=True)\n",
    "    \n",
    "    precisions[model_name] = report['weighted avg']['precision']\n",
    "    recalls[model_name] = report['weighted avg']['recall']\n",
    "    f1_scores[model_name] = report['weighted avg']['f1-score'] \n",
    "\n",
    "    print(\"Classification Report:\\n\", classification_report(ytrue, yhat))\n",
    "   \n",
    "eval_results['precision'] = precisions\n",
    "eval_results['recall'] = recalls\n",
    "eval_results['f1 score'] = f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d39476",
   "metadata": {},
   "source": [
    "# 10. Choose Model to Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d72d0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttnLSTM\n",
    "model_name = 'AttnLSTM'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0015ce",
   "metadata": {},
   "source": [
    "# 11. Calculate Joint Angles & Count Reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f172932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_calculation(a,b,c):\n",
    "    a = np.array(a) \n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    \n",
    "    rad = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(rad*180.0/np.pi)\n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "26f357fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_ordinates(landmarks, mp_pose, side, joint):\n",
    "    coord = getattr(mp_pose.PoseLandmark,side.upper()+\"_\"+joint.upper())\n",
    "    x_val = landmarks[coord.value].x\n",
    "    y_val = landmarks[coord.value].y\n",
    "    return [x_val, y_val]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f11273cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_angle(image, angle, joint):\n",
    "    cv2.putText(image, str(int(angle)), \n",
    "                   tuple(np.multiply(joint, [640, 480]).astype(int)), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                        )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9031f63a-80a5-4919-9065-ba0e104ab205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_reps(image, current_action, landmarks, mp_pose):\n",
    "    global curl_stage, press_stage, squat_stage, butbrdige_stage, leglift_stage,r_leglift_stage,l_butbrdige_stage,situps_stage,curl_counter, press_counter, squat_counter,butbrdige_counter,leglift_counter,situps_counter\n",
    "\n",
    "    if current_action == 'situps':\n",
    "        shoulder_l = co_ordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        hip_l = co_ordinates(landmarks, mp_pose, 'left', 'hip')\n",
    "        knee_l = co_ordinates(landmarks, mp_pose, 'left', 'knee')\n",
    "    \n",
    "        shoulder_r = co_ordinates(landmarks, mp_pose, 'right', 'shoulder')\n",
    "        hip_r = co_ordinates(landmarks, mp_pose, 'right', 'hip')\n",
    "        knee_r = co_ordinates(landmarks, mp_pose, 'right', 'knee')\n",
    "    \n",
    "        angle_left = angle_calculation(shoulder_l, hip_l, knee_l)\n",
    "        angle_right = angle_calculation(shoulder_r, hip_r, knee_r)\n",
    "    \n",
    "        # if angle_left > 160:\n",
    "        #     l_situps_stage = \"up\"\n",
    "        #     print(l_situps_stage, \"Left\")\n",
    "        # if angle_left < 140 and l_situps_stage == \"up\":\n",
    "        #     print(\"Sit-ups counted for left side\")\n",
    "        #     l_situps_stage = \"down\"\n",
    "    \n",
    "        if angle_right <100 :\n",
    "            situps_stage = \"up\"\n",
    "        if angle_right > 120 and situps_stage == \"up\":\n",
    "            situps_stage = \"down\"\n",
    "            situps_counter+=1\n",
    "    \n",
    "        press_stage = None\n",
    "        squat_stage = None\n",
    "        curl_stage = None\n",
    "        leglift_stage = None\n",
    "        r_leglift_stage = None\n",
    "        butbrdige_stage=None\n",
    "        l_butbrdige_stage =None\n",
    "        \n",
    "\n",
    "        show_angle(image, angle_right, hip_r)\n",
    "\n",
    "    elif current_action == 'butbrdige':\n",
    "        shoulder_l = co_ordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        hip_l = co_ordinates(landmarks, mp_pose, 'left', 'hip')\n",
    "        knee_l = co_ordinates(landmarks, mp_pose, 'left', 'knee')\n",
    "\n",
    "        shoulder_r = co_ordinates(landmarks, mp_pose, 'right', 'shoulder')\n",
    "        hip_r = co_ordinates(landmarks, mp_pose, 'right', 'hip')\n",
    "        knee_r = co_ordinates(landmarks, mp_pose, 'right', 'knee')\n",
    "\n",
    "        angle_left = angle_calculation(shoulder_l,hip_l,knee_l)\n",
    "        angle_right = angle_calculation(knee_r,hip_r,shoulder_r)\n",
    "        \n",
    "        # if (angle_right <180 and angle_right > 170):\n",
    "        #     butbrdige_stage=\"up\"\n",
    "        #     print(butbrdige_stage,\"right\")\n",
    "        # if (angle_right <160 and butbrdige_stage==\"up\"):\n",
    "        #     print(\"ddddddd_right\",butbrdige_stage)\n",
    "        #     butbrdige_stage=\"down\"\n",
    "        #     butbrdige_counter+=1\n",
    "\n",
    "        if(angle_left <180 and angle_left > 170) :\n",
    "            l_butbrdige_stage=\"up\"\n",
    "        if(angle_left <160 and l_butbrdige_stage==\"up\"):\n",
    "            l_butbrdige_stage=\"down\"\n",
    "            butbrdige_counter+=1\n",
    "\n",
    "        press_stage = None\n",
    "        squat_stage = None\n",
    "        curl_stage=None\n",
    "        leglift_stage=None\n",
    "        r_leglift_stage=None\n",
    "\n",
    "        show_angle(image, angle_right, hip_r)\n",
    "\n",
    "    elif current_action == 'leglift':\n",
    "\n",
    "        shoulder_l = co_ordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        hip_l = co_ordinates(landmarks, mp_pose, 'left', 'hip')\n",
    "        knee_l = co_ordinates(landmarks, mp_pose, 'left', 'knee')\n",
    "        ankle_l = co_ordinates(landmarks, mp_pose, 'left', 'ankle')\n",
    "\n",
    "        shoulder_r = co_ordinates(landmarks, mp_pose, 'right', 'shoulder')\n",
    "        hip_r = co_ordinates(landmarks, mp_pose, 'right', 'hip')\n",
    "        knee_r = co_ordinates(landmarks, mp_pose, 'right', 'knee')\n",
    "        ankle_r = co_ordinates(landmarks, mp_pose, 'right', 'ankle')\n",
    "\n",
    "        hip_angle_left = angle_calculation(shoulder_l,hip_l,knee_l)\n",
    "        hip_angle_right = angle_calculation(shoulder_r,hip_r,knee_r)\n",
    "\n",
    "        knee_angle_left = angle_calculation(hip_l,knee_l,ankle_l)\n",
    "        knee_angle_right = angle_calculation(hip_r,knee_r,ankle_r)\n",
    "        \n",
    "        d_hip_angle_left = angle_calculation(shoulder_l,hip_l,ankle_l)\n",
    "        d_hip_angle_right = angle_calculation(shoulder_r,hip_r,ankle_r)\n",
    "       \n",
    "\n",
    "        if (hip_angle_left<100 and hip_angle_left > 70) and (knee_angle_left> 40 and knee_angle_left < 90):\n",
    "            leglift_stage = \"up\" \n",
    "            \n",
    "        if d_hip_angle_left>170 and leglift_stage == \"up\" :\n",
    "            leglift_stage=\"down\"\n",
    "            leglift_counter +=1\n",
    "\n",
    "        if (hip_angle_right<100 and hip_angle_right > 70) and (knee_angle_right> 40 and knee_angle_right < 90):\n",
    "            r_leglift_stage = \"up\" \n",
    "    \n",
    "        if d_hip_angle_right>170 and r_leglift_stage == \"up\" :\n",
    "            r_leglift_stage=\"down\"\n",
    "            leglift_counter +=1\n",
    "\n",
    "        butbrdige_stage=None\n",
    "        press_stage = None\n",
    "        squat_stage = None\n",
    "        curl_stage = None\n",
    "        l_butbrdige_stage =None\n",
    "    \n",
    "    elif current_action == 'curl':\n",
    "        shoulder_l = co_ordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        elbow_l = co_ordinates(landmarks, mp_pose, 'left', 'elbow')\n",
    "        wrist_l = co_ordinates(landmarks, mp_pose, 'left', 'wrist')\n",
    "\n",
    "        shoulder_r = co_ordinates(landmarks, mp_pose, 'right', 'shoulder')\n",
    "        elbow_r = co_ordinates(landmarks, mp_pose, 'right', 'elbow')\n",
    "        wrist_r = co_ordinates(landmarks, mp_pose, 'right', 'wrist')\n",
    "        \n",
    "        elbow_angle_l = angle_calculation(shoulder_l, elbow_l, wrist_l)\n",
    "        elbow_angle_r = angle_calculation(shoulder_r, elbow_r, wrist_r)\n",
    "        \n",
    "        if elbow_angle_l < 30 and elbow_angle_r<30:\n",
    "            curl_stage = \"up\" \n",
    "        if elbow_angle_l > 160 and elbow_angle_r>160 and curl_stage =='up':\n",
    "            curl_stage=\"down\"  \n",
    "            curl_counter +=1\n",
    "\n",
    "        butbrdige_stage=None\n",
    "        press_stage = None\n",
    "        squat_stage = None\n",
    "        leglift_stage=None\n",
    "        r_leglift_stage=None\n",
    "        l_butbrdige_stage =None\n",
    "        show_angle(image, elbow_angle_l, elbow_l)\n",
    "        show_angle(image, elbow_angle_r, elbow_r)\n",
    "        \n",
    "    elif current_action == 'press':\n",
    "        \n",
    "        shoulder_l = co_ordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        elbow_l = co_ordinates(landmarks, mp_pose, 'left', 'elbow')\n",
    "        wrist_l = co_ordinates(landmarks, mp_pose, 'left', 'wrist')\n",
    "\n",
    "        shoulder_r = co_ordinates(landmarks, mp_pose, 'right', 'shoulder')\n",
    "        elbow_r = co_ordinates(landmarks, mp_pose, 'right', 'elbow')\n",
    "        wrist_r = co_ordinates(landmarks, mp_pose, 'right', 'wrist')\n",
    "\n",
    "        elbow_angle_l = angle_calculation(shoulder_l, elbow_l, wrist_l)\n",
    "        elbow_angle_r = angle_calculation(shoulder_r, elbow_r, wrist_r)\n",
    "        \n",
    "        if (elbow_angle_l > 140) and (elbow_angle_r > 140) :\n",
    "            press_stage = \"up\"\n",
    "        if (elbow_angle_l< 50 and press_stage =='up') and (elbow_angle_r< 50 and press_stage =='up'):\n",
    "            press_stage='down'\n",
    "            press_counter += 1\n",
    "        butbrdige_stage=None\n",
    "        curl_stage = None\n",
    "        squat_stage = None\n",
    "        leglift_stage=None\n",
    "        r_leglift_stage=None\n",
    "        l_butbrdige_stage =None\n",
    "            \n",
    "        show_angle(image, elbow_angle_l, elbow_l)\n",
    "        show_angle(image, elbow_angle_r, elbow_r)\n",
    "        \n",
    "    elif current_action == 'squat':\n",
    "        \n",
    "        left_shoulder = co_ordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        left_hip = co_ordinates(landmarks, mp_pose, 'left', 'hip')\n",
    "        left_knee = co_ordinates(landmarks, mp_pose, 'left', 'knee')\n",
    "        left_ankle = co_ordinates(landmarks, mp_pose, 'left', 'ankle')\n",
    "        right_shoulder = co_ordinates(landmarks, mp_pose, 'right', 'shoulder')\n",
    "        right_hip = co_ordinates(landmarks, mp_pose, 'right', 'hip')\n",
    "        right_knee = co_ordinates(landmarks, mp_pose, 'right', 'knee')\n",
    "        right_ankle = co_ordinates(landmarks, mp_pose, 'right', 'ankle')\n",
    "        \n",
    "        left_knee_angle = angle_calculation(left_hip, left_knee, left_ankle)\n",
    "        right_knee_angle = angle_calculation(right_hip, right_knee, right_ankle)\n",
    "        \n",
    "        left_hip_angle = angle_calculation(left_shoulder, left_hip, left_knee)\n",
    "        right_hip_angle = angle_calculation(right_shoulder, right_hip, right_knee)\n",
    "        \n",
    "        thr = 165\n",
    "        if (left_knee_angle < thr) and (right_knee_angle < thr) and (left_hip_angle < thr) and (right_hip_angle < thr):\n",
    "            squat_stage = \"down\"\n",
    "        if (left_knee_angle > thr) and (right_knee_angle > thr) and (left_hip_angle > thr) and (right_hip_angle > thr) and (squat_stage =='down'):\n",
    "            squat_stage='up'\n",
    "            squat_counter += 1\n",
    "            \n",
    "        butbrdige_stage=None\n",
    "        curl_stage = None\n",
    "        press_stage = None\n",
    "        leglift_stage=None\n",
    "        r_leglift_stage=None\n",
    "        l_butbrdige_stage =None\n",
    "            \n",
    "        show_angle(image, left_knee_angle, left_knee)\n",
    "        show_angle(image, left_hip_angle, left_hip)\n",
    "        \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5116ef6",
   "metadata": {},
   "source": [
    "# 12. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4775b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):        \n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6332bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "predictions = []\n",
    "i = []\n",
    "threshold = 0.5 \n",
    "current_action = ''\n",
    "\n",
    "curl_counter = 0\n",
    "press_counter = 0\n",
    "squat_counter = 0\n",
    "butbrdige_counter=0\n",
    "leglift_counter=0\n",
    "situps_counter=0\n",
    "\n",
    "curl_stage = None\n",
    "press_stage = None\n",
    "squat_stage = None\n",
    "butbrdige_stage= None\n",
    "leglift_stage=None\n",
    "l_butbrdige_stage=None\n",
    "situps_stage=None\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('M','J','P','G') \n",
    "HEIGHT = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "FPS = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "video_name = os.path.join(os.getcwd(),f\"{model_name}_real_time_test.avi\")\n",
    "out = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*\"MJPG\"), FPS, (WIDTH,HEIGHT))\n",
    "\n",
    "with pose_module.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mp_detection(frame, pose)\n",
    "        draw_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)        \n",
    "        sequence.append(keypoints)      \n",
    "        sequence = sequence[-sequence_length:]\n",
    "              \n",
    "        if len(sequence) == sequence_length:\n",
    "            i = model.predict(np.expand_dims(sequence, axis=0), verbose=0)[0]           \n",
    "            predictions.append(np.argmax(i))\n",
    "            current_action = actions[np.argmax(i)]\n",
    "            confidence = np.max(i)\n",
    "            \n",
    "            if confidence < threshold:\n",
    "                current_action = ''\n",
    "            image = probability(i, actions, image, colors)\n",
    "            \n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                count_reps(\n",
    "                    image, current_action, landmarks, pose_module)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            cv2.rectangle(image, (0,0), (1400, 40), colors[np.argmax(i)], -1)\n",
    "            cv2.putText(image, 'curl ' + str(curl_counter), (5,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'press ' + str(press_counter), (225,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'squat ' + str(squat_counter), (450,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'butbrdige ' + str(butbrdige_counter), (625,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'leglift ' + str(leglift_counter), (825,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'situps ' + str(situps_counter), (1020,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "            \n",
    "             \n",
    "        cv2.imshow('OpenCV Feed', image)       \n",
    "        if ret == True:\n",
    "            out.write(image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9980a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "80aa1d3f3a8cfb37a38c47373cc49a39149184c5fa770d709389b1b8782c1d85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
